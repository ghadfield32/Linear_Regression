{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load data\n",
    "# Reads a comma-separated values (CSV) file into a DataFrame\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('target_column', axis=1)  # Removes target_column, leaving feature columns\n",
    "y = data['target_column']  # Selects target_column as the target variable\n",
    "\n",
    "# Handle missing values\n",
    "# Get numerical and categorical features based on their data types\n",
    "numerical_features = [column for column in X.columns if X[column].dtype != 'object']\n",
    "categorical_features = [column for column in X.columns if X[column].dtype == 'object']\n",
    "\n",
    "# Define numerical transformer\n",
    "# A pipeline that first fills missing numerical values with the mean, then scales them to have mean=0 and variance=1\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fills missing numerical values with the mean\n",
    "    ('scaler', StandardScaler())                 # Scales numerical features\n",
    "])\n",
    "\n",
    "# Define categorical transformer\n",
    "# A pipeline that first fills missing categorical values with the most frequent category, then one-hot encodes them\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fills missing categorical values with the most frequent category\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))    # One-hot encodes categorical features\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical transformers\n",
    "# Applies the numerical and categorical transformations defined above to the corresponding columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data\n",
    "# Splits the data into training (80%) and testing (20%) sets, with a random seed for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing on training data\n",
    "# Fits and transforms the training data using the preprocessor defined above\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Apply preprocessing on testing data\n",
    "# Transforms the testing data using the same preprocessor (without fitting, to prevent data leakage)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBA Data Preprocessing Template\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the columns for the dependent (y) and independent (X) variables\n",
    "y_column = 'WL_encoded'\n",
    "x_excluded_independent_variables = ['GAME_DATE','TEAM_ABBREVIATION','WL','ORB%','DRB%','FGA','TOV','TOV%', 'TEAM_NAME','MATCHUP',\n",
    "                                    'USG%','FG3A','OREB','PF','MIN','PTS','FGM','FG3M','FTM','FTA','PTS','REB','AST','PF','STL','BLK','DREB']\n",
    "\n",
    "# Combine y_column with x_excluded_independent_variables\n",
    "columns_to_drop = [y_column] + x_excluded_independent_variables\n",
    "\n",
    "# Separate features and target\n",
    "X = season_data.drop(columns=columns_to_drop, axis=1)  # Corrected, removed extra square brackets\n",
    "y = season_data['WL_encoded']  # Selects target_column as the target variable\n",
    "\n",
    "# Handle missing values\n",
    "# Get numerical and categorical features based on their data types\n",
    "numerical_features = [column for column in X.columns if X[column].dtype != 'object']\n",
    "categorical_features = [column for column in X.columns if X[column].dtype == 'object']\n",
    "\n",
    "# Define numerical transformer\n",
    "# A pipeline that first fills missing numerical values with the mean, then scales them to have mean=0 and variance=1\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fills missing numerical values with the mean\n",
    "    ('scaler', StandardScaler())                 # Scales numerical features\n",
    "])\n",
    "\n",
    "# Define categorical transformer\n",
    "# A pipeline that first fills missing categorical values with the most frequent category, then one-hot encodes them\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fills missing categorical values with the most frequent category\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))    # One-hot encodes categorical features\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical transformers\n",
    "# Applies the numerical and categorical transformations defined above to the corresponding columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "print(X.head())\n",
    "# Split data\n",
    "# Splits the data into training (80%) and testing (20%) sets, with a random seed for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing on training data\n",
    "# Fits and transforms the training data using the preprocessor defined above\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Apply preprocessing on testing data\n",
    "# Transforms the testing data using the same preprocessor (without fitting, to prevent data leakage)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
