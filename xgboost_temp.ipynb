{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Assuming you have already prepared the X (features) and y (target) data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the XGBoost classifier model\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Hyperparameter tuning options\n",
    "# Uncomment one of the following blocks to choose the tuning method\n",
    "\n",
    "# Option 1: Grid Search (specify the hyperparameter grid)\n",
    "#Grid Search: This method searches exhaustively through a specified parameter grid. It provides a comprehensive exploration of the hyperparameter space but can be computationally expensive, especially with many hyperparameters.\n",
    "# param_grid = {\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'learning_rate': [0.1, 0.01, 0.001],\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'min_child_weight': [1, 3, 5],\n",
    "#     'gamma': [0, 0.1, 0.2]\n",
    "# }\n",
    "# grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Option 2: Randomized Search (specify the hyperparameter distributions)\n",
    "#Randomized Search: This method searches randomly through a specified parameter distribution. It provides a good exploration of the hyperparameter space and is less computationally expensive than Grid Search. However, it may not find the optimal hyperparameters.\n",
    "# param_distributions = {\n",
    "#     'max_depth': [3, 5, 7, 10],\n",
    "#     'learning_rate': [0.1, 0.01, 0.001],\n",
    "#     'n_estimators': [100, 200, 300, 400],\n",
    "#     'min_child_weight': [1, 3, 5],\n",
    "#     'gamma': [0, 0.1, 0.2, 0.3, 0.4]\n",
    "# }\n",
    "# randomized_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_distributions,\n",
    "#                                        scoring='accuracy', cv=5, n_iter=10, random_state=42)\n",
    "# randomized_search.fit(X_train, y_train)\n",
    "# xgb_model = randomized_search.best_estimator_\n",
    "\n",
    "# Note: Uncomment only one of the above options for hyperparameter tuning.\n",
    "\n",
    "# Train the model on the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for XGBoost\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "xgb_precision = precision_score(y_test, y_pred_xgb)\n",
    "xgb_recall = recall_score(y_test, y_pred_xgb)\n",
    "xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
    "xgb_confusion_matrix = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)\n",
    "print(\"XGBoost Precision:\", xgb_precision)\n",
    "print(\"XGBoost Recall:\", xgb_recall)\n",
    "print(\"XGBoost F1-score:\", xgb_f1)\n",
    "print(\"XGBoost Confusion Matrix:\")\n",
    "print(xgb_confusion_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
